{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DzWmBT__dPIV",
        "outputId": "09917a69-dc25-42e7-83eb-4fda1a5a63fd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                     Accuracy  Precision    Recall  Type I Error  \\\n",
            "Logistic Regression   0.67325   0.673250  1.000000      1.000000   \n",
            "Decision Tree         0.56425   0.687599  0.646491      0.605203   \n",
            "Random Forest         0.69950   0.712091  0.929447      0.774292   \n",
            "Gradient Boosting     0.70100   0.712461  0.932046      0.775057   \n",
            "SVM                   0.67325   0.673250  1.000000      1.000000   \n",
            "KNN                   0.59000   0.671220  0.766431      0.773527   \n",
            "\n",
            "                     Type II Error  \n",
            "Logistic Regression       0.000000  \n",
            "Decision Tree             0.353509  \n",
            "Random Forest             0.070553  \n",
            "Gradient Boosting         0.067954  \n",
            "SVM                       0.000000  \n",
            "KNN                       0.233569  \n",
            "Z-Test for Mean Age Difference: Z-Statistic=0.6699948763891681, P-Value=0.5028610563663367\n",
            "Z-Test for Type I Error in Random Forest: Z-Statistic=nan, P-Value=nan\n",
            "Z-Test for Type II Error between SVM and KNN: Z-Statistic=nan, P-Value=nan\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.11/dist-packages/statsmodels/stats/weightstats.py:1559: RuntimeWarning: invalid value encountered in scalar divide\n",
            "  var = x1_var / (nobs1 - ddof)\n",
            "/usr/local/lib/python3.11/dist-packages/statsmodels/stats/weightstats.py:1554: RuntimeWarning: invalid value encountered in scalar divide\n",
            "  var /= nobs1 + nobs2 - 2 * ddof\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, confusion_matrix\n",
        "from statsmodels.stats.weightstats import ztest\n",
        "df = pd.read_csv(\"/content/Dry_Eye_Dataset.csv\")\n",
        "label_encoders = {}\n",
        "for col in df.columns:\n",
        "    if df[col].dtype == 'object':\n",
        "        le = LabelEncoder()\n",
        "        df[col] = le.fit_transform(df[col])\n",
        "        label_encoders[col] = le\n",
        "X = df.drop(columns=['Dry Eye Disease']) # Replace 'class' with the actual target column name\n",
        "y = df['Dry Eye Disease'] # Replace 'class'\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "models = {\n",
        "    \"Logistic Regression\": LogisticRegression(),\n",
        "    \"Decision Tree\": DecisionTreeClassifier(),\n",
        "    \"Random Forest\": RandomForestClassifier(),\n",
        "    \"Gradient Boosting\": GradientBoostingClassifier(),\n",
        "    \"SVM\": SVC(probability=True),\n",
        "    \"KNN\": KNeighborsClassifier()\n",
        "}\n",
        "results = {}\n",
        "for name, model in models.items():\n",
        "    model.fit(X_train, y_train)\n",
        "    y_pred = model.predict(X_test)\n",
        "\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    precision = precision_score(y_test, y_pred)\n",
        "    recall = recall_score(y_test, y_pred)\n",
        "    tn, fp, fn, tp = confusion_matrix(y_test, y_pred).ravel()\n",
        "    type_i_error = fp / (fp + tn)\n",
        "    type_ii_error = fn / (fn + tp)\n",
        "\n",
        "    results[name] = {\n",
        "        \"Accuracy\": accuracy,\n",
        "        \"Precision\": precision,\n",
        "        \"Recall\": recall,\n",
        "        \"Type I Error\": type_i_error,\n",
        "        \"Type II Error\": type_ii_error\n",
        "    }\n",
        "results_df = pd.DataFrame(results).T\n",
        "print(results_df)\n",
        "log_reg = LogisticRegression()\n",
        "log_reg.fit(X_train, y_train)\n",
        "y_pred = log_reg.predict(X_test)\n",
        "misclassified_indices = (y_test != y_pred)\n",
        "correctly_classified_ages = X_test.loc[~misclassified_indices, \"Age\"]\n",
        "misclassified_ages = X_test.loc[misclassified_indices, \"Age\"]\n",
        "z_stat, p_value = ztest(correctly_classified_ages, misclassified_ages)\n",
        "print(f\"Z-Test for Mean Age Difference: Z-Statistic={z_stat}, P-Value={p_value}\")\n",
        "rf_fp_rate = results[\"Random Forest\"][\"Type I Error\"]\n",
        "if rf_fp_rate > 0.20:\n",
        "    z_stat, p_value = ztest([rf_fp_rate], value=0.20)\n",
        "    print(f\"Z-Test for Type I Error in Random Forest: Z-Statistic={z_stat}, P-Value={p_value}\")\n",
        "z_stat, p_value = ztest([results[\"SVM\"][\"Type II Error\"]], [results[\"KNN\"][\"Type II Error\"]])\n",
        "print(f\"Z-Test for Type II Error between SVM and KNN: Z-Statistic={z_stat}, P-Value={p_value}\")\n"
      ]
    }
  ]
}